{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "MBFNlYodjcQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow scikit-learn numpy torch joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcgKIwzmlfjM",
        "outputId": "0a1f3334-3212-4214-b551-fe95197dd6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.25.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_dists.py\", line 225, in iter_dependencies\n",
            "    elif not extras and req.marker.evaluate({\"extra\": \"\"}):\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/markers.py\", line 325, in evaluate\n",
            "    return _evaluate_markers(self._markers, current_environment)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/markers.py\", line 224, in _evaluate_markers\n",
            "    lhs_value, rhs_value = _normalize(lhs_value, rhs_value, key=environment_key)\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/markers.py\", line 198, in _normalize\n",
            "    return tuple(canonicalize_name(v) for v in values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/markers.py\", line 198, in <genexpr>\n",
            "    return tuple(canonicalize_name(v) for v in values)\n",
            "\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1576, in critical\n",
            "    def critical(self, msg, *args, **kwargs):\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "# For Data Preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# For Building the Model\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# For Evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"All libraries imported successfully.\")\n",
        "\n",
        "# --- 1. Feature Engineering Function (Same as before) ---\n",
        "def process_single_sequence(sequence):\n",
        "    \"\"\"\n",
        "    Applies feature engineering to a single video sequence.\n",
        "    \"\"\"\n",
        "    n_frames, n_features = sequence.shape\n",
        "    num_joints = n_features // 2\n",
        "    try:\n",
        "        joints = sequence.reshape(n_frames, num_joints, 2)\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "    center_of_mass = np.mean(joints, axis=1, keepdims=True)\n",
        "    relative_joints = joints - center_of_mass\n",
        "\n",
        "    velocity = np.diff(relative_joints, axis=0)\n",
        "    velocity = np.pad(velocity, ((1, 0), (0, 0), (0, 0)), 'constant')\n",
        "\n",
        "    acceleration = np.diff(velocity, axis=0)\n",
        "    acceleration = np.pad(acceleration, ((1, 0), (0, 0), (0, 0)), 'constant')\n",
        "\n",
        "    combined_features = np.concatenate(\n",
        "        (relative_joints, velocity, acceleration), axis=-1\n",
        "    )\n",
        "\n",
        "    return combined_features.reshape(n_frames, -1)\n",
        "\n",
        "# --- 2. NEW SLICING Data Loader ---\n",
        "def load_data_sliced_and_engineered(pkl_path, json_path):\n",
        "    \"\"\"\n",
        "    Loads data by slicing the PKL tensor based on JSON start/end frames\n",
        "    and then applies feature engineering.\n",
        "    \"\"\"\n",
        "    print(f\"Loading {json_path} and {pkl_path}...\")\n",
        "    with open(pkl_path, 'rb') as f:\n",
        "        pkl_data = pickle.load(f)\n",
        "    with open(json_path, 'r') as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    # Create a lookup dictionary from the PKL data for fast access\n",
        "    # This is safer than assuming they are in the same order\n",
        "    pkl_lookup = {item['video_name']: item['coordinates'].numpy() for item in pkl_data}\n",
        "\n",
        "    X_engineered = []\n",
        "    y_labels = []\n",
        "\n",
        "    for item in json_data:\n",
        "        video_name = item['video_name']\n",
        "        if video_name in pkl_lookup:\n",
        "            full_coords = pkl_lookup[video_name]\n",
        "\n",
        "            # --- THIS IS THE CRITICAL SLICING STEP ---\n",
        "            start = item['gt_start_frame']\n",
        "            end = item['gt_end_frame']\n",
        "            sliced_coords = full_coords[start:end]\n",
        "            # -------------------------------------------\n",
        "\n",
        "            if sliced_coords.shape[0] > 0: # Ensure slice is not empty\n",
        "                # Now, apply feature engineering to the *clean* slice\n",
        "                engineered_features = process_single_sequence(sliced_coords)\n",
        "\n",
        "                if engineered_features is not None:\n",
        "                    X_engineered.append(engineered_features)\n",
        "                    y_labels.append(item['motion_type'])\n",
        "\n",
        "    print(f\"Successfully loaded and sliced {len(X_engineered)} samples.\")\n",
        "    return X_engineered, y_labels\n",
        "\n",
        "# --- 3. Execute Loading and Preprocessing ---\n",
        "X_train_list, y_train_list = load_data_sliced_and_engineered('FS_train.pkl', 'FS_train.json')\n",
        "X_test_list, y_test_list = load_data_sliced_and_engineered('FS_test.pkl', 'FS_test.json')\n",
        "\n",
        "# --- 4. Preprocess Labels (Same as before) ---\n",
        "print(\"\\nPreprocessing labels...\")\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train_list)\n",
        "y_test_encoded = label_encoder.transform(y_test_list)\n",
        "\n",
        "n_classes = len(label_encoder.classes_)\n",
        "print(f\"Found {n_classes} unique classes.\")\n",
        "\n",
        "y_train_one_hot = to_categorical(y_train_encoded, num_classes=n_classes)\n",
        "y_test_one_hot = to_categorical(y_test_encoded, num_classes=n_classes)\n",
        "\n",
        "# --- 5. Preprocess Features (Scaling) ---\n",
        "print(\"\\nPreprocessing features (scaling)...\")\n",
        "scaler = StandardScaler()\n",
        "all_train_frames = np.vstack(X_train_list)\n",
        "print(f\"Total training frames for scaling: {all_train_frames.shape}\")\n",
        "scaler.fit(all_train_frames)\n",
        "\n",
        "X_train_scaled = [scaler.transform(seq) for seq in X_train_list]\n",
        "X_test_scaled = [scaler.transform(seq) for seq in X_test_list]\n",
        "\n",
        "# --- 6. Preprocess Features (Padding) ---\n",
        "print(\"\\nPreprocessing features (padding)...\")\n",
        "max_len_train = max(len(seq) for seq in X_train_scaled)\n",
        "max_len_test = max(len(seq) for seq in X_test_scaled)\n",
        "MAX_SEQUENCE_LENGTH = max(max_len_train, max_len_test)\n",
        "print(f\"New MAX_SEQUENCE_LENGTH (based on slices): {MAX_SEQUENCE_LENGTH}\")\n",
        "\n",
        "n_features = X_train_scaled[0].shape[1]\n",
        "print(f\"Number of engineered features: {n_features}\") # Should be 150\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_scaled, maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                               padding='post', dtype='float32')\n",
        "X_test_padded = pad_sequences(X_test_scaled, maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                              padding='post', dtype='float32')\n",
        "\n",
        "print(f\"\\nShape of final padded training data: {X_train_padded.shape}\")\n",
        "print(f\"Shape of final padded testing data: {X_test_padded.shape}\")\n",
        "print(\"\\n--- Preprocessing Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-8PvRwjqf7J",
        "outputId": "a4fb5b0e-49bb-4dea-a634-3379e48a6d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully.\n",
            "Loading FS_train.json and FS_train.pkl...\n",
            "Successfully loaded and sliced 287 samples.\n",
            "Loading FS_test.json and FS_test.pkl...\n",
            "Successfully loaded and sliced 64 samples.\n",
            "\n",
            "Preprocessing labels...\n",
            "Found 4 unique classes.\n",
            "\n",
            "Preprocessing features (scaling)...\n",
            "Total training frames for scaling: (7018, 198)\n",
            "\n",
            "Preprocessing features (padding)...\n",
            "New MAX_SEQUENCE_LENGTH (based on slices): 61\n",
            "Number of engineered features: 198\n",
            "\n",
            "Shape of final padded training data: (287, 61, 198)\n",
            "Shape of final padded testing data: (64, 61, 198)\n",
            "\n",
            "--- Preprocessing Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def augment_jitter(sequence, amount=0.002):\n",
        "    \"\"\"\n",
        "    Adds small random noise to a sequence.\n",
        "    \"\"\"\n",
        "    noise = np.random.normal(0, amount, sequence.shape)\n",
        "    return sequence + noise\n",
        "\n",
        "def augment_scale(sequence):\n",
        "    \"\"\"\n",
        "    Scales the skeleton by a random factor between 0.95 and 1.05.\n",
        "    Assumes engineered features (Pos, Vel, Accel).\n",
        "\n",
        "    We only scale the POSITIONAL data (the first 1/3 of features).\n",
        "    Velocity and Acceleration are more complex to scale, so we'll\n",
        "    leave them as-is for simplicity.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original\n",
        "    new_seq = np.copy(sequence)\n",
        "\n",
        "    # n_features will be 150. We only want the first 50 (Pos_x, Pos_y, ...)\n",
        "    # The reshape from (150) -> (25, 6) put all Pos data first\n",
        "    # So the first 1/3 of features are all positional.\n",
        "    n_features = new_seq.shape[1]\n",
        "    pos_features = n_features // 3\n",
        "\n",
        "    # Pick a random scaling factor\n",
        "    scale_factor = np.random.uniform(0.95, 1.05)\n",
        "\n",
        "    # Apply scaling only to the positional features\n",
        "    new_seq[:, :pos_features] = new_seq[:, :pos_features] * scale_factor\n",
        "\n",
        "    return new_seq"
      ],
      "metadata": {
        "id": "EFL9_tvJrZ2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Load Sliced and Engineered Data (Same as before) ---\n",
        "X_train_list, y_train_list = load_data_sliced_and_engineered('FS_train.pkl', 'FS_train.json')\n",
        "X_test_list, y_test_list = load_data_sliced_and_engineered('FS_test.pkl', 'FS_test.json')\n",
        "\n",
        "# --- 2. NEW: DATA AUGMENTATION ---\n",
        "print(f\"\\nOriginal training samples: {len(X_train_list)}\")\n",
        "\n",
        "X_train_augmented = []\n",
        "y_train_augmented = []\n",
        "\n",
        "for seq, label in zip(X_train_list, y_train_list):\n",
        "    # 1. Add the original, unmodified sequence\n",
        "    X_train_augmented.append(seq)\n",
        "    y_train_augmented.append(label)\n",
        "\n",
        "    # 2. Add a 'jittered' version\n",
        "    X_train_augmented.append(augment_jitter(seq))\n",
        "    y_train_augmented.append(label)\n",
        "\n",
        "    # 3. Add a 'scaled' version\n",
        "    X_train_augmented.append(augment_scale(seq))\n",
        "    y_train_augmented.append(label)\n",
        "\n",
        "print(f\"New training samples with augmentation: {len(X_train_augmented)}\")\n",
        "\n",
        "# --- 3. Preprocess Labels ---\n",
        "print(\"\\nPreprocessing labels...\")\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train_augmented) # Use augmented list\n",
        "y_test_encoded = label_encoder.transform(y_test_list) # Test list is unchanged\n",
        "\n",
        "n_classes = len(label_encoder.classes_)\n",
        "print(f\"Found {n_classes} unique classes.\")\n",
        "\n",
        "y_train_one_hot = to_categorical(y_train_encoded, num_classes=n_classes)\n",
        "y_test_one_hot = to_categorical(y_test_encoded, num_classes=n_classes)\n",
        "\n",
        "# --- 4. Preprocess Features (Scaling) ---\n",
        "print(\"\\nPreprocessing features (scaling)...\")\n",
        "scaler = StandardScaler()\n",
        "# **IMPORTANT**: Fit the scaler ONLY on the *augmented training data*\n",
        "all_train_frames = np.vstack(X_train_augmented)\n",
        "print(f\"Total training frames for scaling: {all_train_frames.shape}\")\n",
        "scaler.fit(all_train_frames)\n",
        "\n",
        "# Apply scaler to both train and test\n",
        "X_train_scaled = [scaler.transform(seq) for seq in X_train_augmented]\n",
        "X_test_scaled = [scaler.transform(seq) for seq in X_test_list] # Test list is unchanged\n",
        "\n",
        "# --- 5. Preprocess Features (Padding) ---\n",
        "print(\"\\nPreprocessing features (padding)...\")\n",
        "max_len_train = max(len(seq) for seq in X_train_scaled)\n",
        "max_len_test = max(len(seq) for seq in X_test_list)\n",
        "MAX_SEQUENCE_LENGTH = max(max_len_train, max_len_test)\n",
        "print(f\"New MAX_SEQUENCE_LENGTH: {MAX_SEQUENCE_LENGTH}\")\n",
        "\n",
        "n_features = X_train_scaled[0].shape[1]\n",
        "print(f\"Number of engineered features: {n_features}\")\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_scaled, maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                               padding='post', dtype='float32')\n",
        "X_test_padded = pad_sequences(X_test_scaled, maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                              padding='post', dtype='float32')\n",
        "\n",
        "print(f\"\\nShape of final padded training data: {X_train_padded.shape}\")\n",
        "print(f\"Shape of final padded testing data: {X_test_padded.shape}\")\n",
        "print(\"\\n--- Preprocessing Complete with Augmentation ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRU2SEd4rbRx",
        "outputId": "3c8567be-24d7-4342-9436-a83435f6fc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading FS_train.json and FS_train.pkl...\n",
            "Successfully loaded and sliced 287 samples.\n",
            "Loading FS_test.json and FS_test.pkl...\n",
            "Successfully loaded and sliced 64 samples.\n",
            "\n",
            "Original training samples: 287\n",
            "New training samples with augmentation: 861\n",
            "\n",
            "Preprocessing labels...\n",
            "Found 4 unique classes.\n",
            "\n",
            "Preprocessing features (scaling)...\n",
            "Total training frames for scaling: (21054, 198)\n",
            "\n",
            "Preprocessing features (padding)...\n",
            "New MAX_SEQUENCE_LENGTH: 61\n",
            "Number of engineered features: 198\n",
            "\n",
            "Shape of final padded training data: (861, 61, 198)\n",
            "Shape of final padded testing data: (64, 61, 198)\n",
            "\n",
            "--- Preprocessing Complete with Augmentation ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    LSTM, Dense, Dropout, BatchNormalization, Input, Layer,\n",
        "    GlobalAveragePooling1D, MultiHeadAttention, LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# First, define the Transformer \"Block\" as a custom layer\n",
        "class TransformerBlock(Layer):\n",
        "    # No changes needed here, keeping the structure generic\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "print(\"Building Transformer model...\")\n",
        "\n",
        "# We use the Keras Functional API to build this\n",
        "inputs = Input(shape=(MAX_SEQUENCE_LENGTH, n_features))\n",
        "\n",
        "# ╠ CHANGE 1: INCREASE EMBEDDING DIMENSION (Model Width)\n",
        "embed_dim = 128  # Original: 64\n",
        "ff_dim = 128     # Original: 64\n",
        "num_heads = 4    # Keep at 4\n",
        "\n",
        "x = Dense(embed_dim)(inputs)\n",
        "\n",
        "# ╠ CHANGE 2: INCREASE TRANSFORMER DEPTH (Model Capacity)\n",
        "transformer_block_1 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block_1(x)\n",
        "transformer_block_2 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block_2(x)\n",
        "# ADDED BLOCK 3\n",
        "transformer_block_3 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block_3(x)\n",
        "\n",
        "\n",
        "# Classifier Head\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "# ╠ CHANGE 3: INCREASE DROPOUT FOR REGULARIZATION\n",
        "x = Dropout(0.5)(x) # Original: 0.3\n",
        "x = Dense(32, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x) # Original: 0.3\n",
        "outputs = Dense(n_classes, activation=\"softmax\")(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# --- Step 7: Compile and Train the Transformer Model ---\n",
        "\n",
        "print(\"\\nCompiling and training the Transformer model...\")\n",
        "\n",
        "# ╠ CHANGE 4: DECREASE LEARNING RATE\n",
        "opt = Adam(learning_rate=0.0001) # Original: 0.0005\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# --- Callbacks (Minor Change) ---\n",
        "early_stop = EarlyStopping(monitor='val_accuracy',\n",
        "                           patience=30, # Original: 20\n",
        "                           restore_best_weights=True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy',\n",
        "                              factor=0.2,\n",
        "                              patience=10, # Original: 5\n",
        "                              min_lr=0.000001) # Original: 0.00001\n",
        "\n",
        "# --- Training Settings (No change) ---\n",
        "EPOCHS = 150\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# --- Fit the model (No change) ---\n",
        "history = model.fit(\n",
        "    X_train_padded,\n",
        "    y_train_one_hot,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_test_padded, y_test_one_hot),\n",
        "    callbacks=[early_stop, reduce_lr]\n",
        ")\n",
        "\n",
        "print(\"\\n--- Training complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YOFveZr1uQD2",
        "outputId": "8c935541-200b-4468-b3f0-20d038f80e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building Transformer model...\n",
            "\n",
            "Compiling and training the Transformer model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_53\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_53\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_50 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m198\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_114 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m25,472\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_36            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m297,344\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_37            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m297,344\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_38            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m297,344\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_14     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_145 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_121 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_146 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_122 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m132\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_114 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,472</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_36            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">297,344</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_37            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">297,344</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block_38            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">297,344</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_14     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_145 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_121 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_146 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_122 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m921,764\u001b[0m (3.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">921,764</span> (3.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m921,764\u001b[0m (3.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">921,764</span> (3.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 605ms/step - accuracy: 0.2538 - loss: 1.9483 - val_accuracy: 0.4219 - val_loss: 1.1865 - learning_rate: 1.0000e-04\n",
            "Epoch 2/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 573ms/step - accuracy: 0.3651 - loss: 1.3728 - val_accuracy: 0.4688 - val_loss: 1.1133 - learning_rate: 1.0000e-04\n",
            "Epoch 3/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 602ms/step - accuracy: 0.4009 - loss: 1.2459 - val_accuracy: 0.5469 - val_loss: 0.9905 - learning_rate: 1.0000e-04\n",
            "Epoch 4/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 563ms/step - accuracy: 0.4470 - loss: 1.2396 - val_accuracy: 0.6719 - val_loss: 0.9666 - learning_rate: 1.0000e-04\n",
            "Epoch 5/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 566ms/step - accuracy: 0.4907 - loss: 1.1358 - val_accuracy: 0.6719 - val_loss: 0.8831 - learning_rate: 1.0000e-04\n",
            "Epoch 6/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 581ms/step - accuracy: 0.4830 - loss: 1.1423 - val_accuracy: 0.7031 - val_loss: 0.8988 - learning_rate: 1.0000e-04\n",
            "Epoch 7/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 571ms/step - accuracy: 0.4938 - loss: 1.0555 - val_accuracy: 0.7344 - val_loss: 0.8587 - learning_rate: 1.0000e-04\n",
            "Epoch 8/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 623ms/step - accuracy: 0.5631 - loss: 1.0170 - val_accuracy: 0.7031 - val_loss: 0.7787 - learning_rate: 1.0000e-04\n",
            "Epoch 9/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 606ms/step - accuracy: 0.5718 - loss: 0.9614 - val_accuracy: 0.6250 - val_loss: 0.8646 - learning_rate: 1.0000e-04\n",
            "Epoch 10/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 569ms/step - accuracy: 0.6197 - loss: 0.8873 - val_accuracy: 0.6562 - val_loss: 0.7594 - learning_rate: 1.0000e-04\n",
            "Epoch 11/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 573ms/step - accuracy: 0.6640 - loss: 0.7974 - val_accuracy: 0.6562 - val_loss: 0.7576 - learning_rate: 1.0000e-04\n",
            "Epoch 12/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 572ms/step - accuracy: 0.6755 - loss: 0.7931 - val_accuracy: 0.7031 - val_loss: 0.7335 - learning_rate: 1.0000e-04\n",
            "Epoch 13/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 574ms/step - accuracy: 0.7117 - loss: 0.7325 - val_accuracy: 0.7188 - val_loss: 0.7181 - learning_rate: 1.0000e-04\n",
            "Epoch 14/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 571ms/step - accuracy: 0.7052 - loss: 0.6847 - val_accuracy: 0.6875 - val_loss: 0.7215 - learning_rate: 1.0000e-04\n",
            "Epoch 15/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 608ms/step - accuracy: 0.7566 - loss: 0.6107 - val_accuracy: 0.7031 - val_loss: 0.8713 - learning_rate: 1.0000e-04\n",
            "Epoch 16/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 561ms/step - accuracy: 0.7934 - loss: 0.5397 - val_accuracy: 0.7656 - val_loss: 0.7131 - learning_rate: 1.0000e-04\n",
            "Epoch 17/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 806ms/step - accuracy: 0.8081 - loss: 0.4875 - val_accuracy: 0.6719 - val_loss: 0.7789 - learning_rate: 1.0000e-04\n",
            "Epoch 18/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 561ms/step - accuracy: 0.8447 - loss: 0.4560 - val_accuracy: 0.7031 - val_loss: 0.8697 - learning_rate: 1.0000e-04\n",
            "Epoch 19/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 547ms/step - accuracy: 0.8700 - loss: 0.3876 - val_accuracy: 0.7500 - val_loss: 0.8314 - learning_rate: 1.0000e-04\n",
            "Epoch 20/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 579ms/step - accuracy: 0.8900 - loss: 0.3164 - val_accuracy: 0.7344 - val_loss: 0.7582 - learning_rate: 1.0000e-04\n",
            "Epoch 21/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 579ms/step - accuracy: 0.8689 - loss: 0.3190 - val_accuracy: 0.7188 - val_loss: 0.7574 - learning_rate: 1.0000e-04\n",
            "Epoch 22/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 562ms/step - accuracy: 0.8957 - loss: 0.2692 - val_accuracy: 0.7344 - val_loss: 0.8040 - learning_rate: 1.0000e-04\n",
            "Epoch 23/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 589ms/step - accuracy: 0.9026 - loss: 0.2667 - val_accuracy: 0.7344 - val_loss: 0.9781 - learning_rate: 1.0000e-04\n",
            "Epoch 24/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 566ms/step - accuracy: 0.9270 - loss: 0.2159 - val_accuracy: 0.7188 - val_loss: 0.8035 - learning_rate: 1.0000e-04\n",
            "Epoch 25/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 567ms/step - accuracy: 0.9487 - loss: 0.1589 - val_accuracy: 0.7188 - val_loss: 1.0708 - learning_rate: 1.0000e-04\n",
            "Epoch 26/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 563ms/step - accuracy: 0.9393 - loss: 0.1888 - val_accuracy: 0.6875 - val_loss: 1.1084 - learning_rate: 1.0000e-04\n",
            "Epoch 27/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 581ms/step - accuracy: 0.9432 - loss: 0.1812 - val_accuracy: 0.7188 - val_loss: 0.9675 - learning_rate: 2.0000e-05\n",
            "Epoch 28/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 548ms/step - accuracy: 0.9670 - loss: 0.1069 - val_accuracy: 0.7812 - val_loss: 0.9434 - learning_rate: 2.0000e-05\n",
            "Epoch 29/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 553ms/step - accuracy: 0.9679 - loss: 0.1231 - val_accuracy: 0.7500 - val_loss: 0.9266 - learning_rate: 2.0000e-05\n",
            "Epoch 30/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 563ms/step - accuracy: 0.9723 - loss: 0.0936 - val_accuracy: 0.7656 - val_loss: 0.9175 - learning_rate: 2.0000e-05\n",
            "Epoch 31/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 554ms/step - accuracy: 0.9755 - loss: 0.1135 - val_accuracy: 0.7500 - val_loss: 0.9337 - learning_rate: 2.0000e-05\n",
            "Epoch 32/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 604ms/step - accuracy: 0.9564 - loss: 0.1112 - val_accuracy: 0.7500 - val_loss: 0.9208 - learning_rate: 2.0000e-05\n",
            "Epoch 33/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 546ms/step - accuracy: 0.9644 - loss: 0.1224 - val_accuracy: 0.7656 - val_loss: 0.9486 - learning_rate: 2.0000e-05\n",
            "Epoch 34/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 549ms/step - accuracy: 0.9748 - loss: 0.0839 - val_accuracy: 0.7656 - val_loss: 0.9611 - learning_rate: 2.0000e-05\n",
            "Epoch 35/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 552ms/step - accuracy: 0.9757 - loss: 0.0983 - val_accuracy: 0.7500 - val_loss: 0.9441 - learning_rate: 2.0000e-05\n",
            "Epoch 36/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 548ms/step - accuracy: 0.9759 - loss: 0.0903 - val_accuracy: 0.7500 - val_loss: 1.0034 - learning_rate: 2.0000e-05\n",
            "Epoch 37/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 556ms/step - accuracy: 0.9821 - loss: 0.0911 - val_accuracy: 0.7344 - val_loss: 1.0441 - learning_rate: 2.0000e-05\n",
            "Epoch 38/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 573ms/step - accuracy: 0.9781 - loss: 0.0979 - val_accuracy: 0.7812 - val_loss: 0.9834 - learning_rate: 2.0000e-05\n",
            "Epoch 39/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 550ms/step - accuracy: 0.9771 - loss: 0.0752 - val_accuracy: 0.7812 - val_loss: 0.9771 - learning_rate: 4.0000e-06\n",
            "Epoch 40/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 570ms/step - accuracy: 0.9848 - loss: 0.0791 - val_accuracy: 0.7812 - val_loss: 0.9736 - learning_rate: 4.0000e-06\n",
            "Epoch 41/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 562ms/step - accuracy: 0.9601 - loss: 0.1087 - val_accuracy: 0.7656 - val_loss: 0.9832 - learning_rate: 4.0000e-06\n",
            "Epoch 42/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 553ms/step - accuracy: 0.9777 - loss: 0.0889 - val_accuracy: 0.7656 - val_loss: 0.9920 - learning_rate: 4.0000e-06\n",
            "Epoch 43/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 545ms/step - accuracy: 0.9751 - loss: 0.0914 - val_accuracy: 0.7656 - val_loss: 0.9874 - learning_rate: 4.0000e-06\n",
            "Epoch 44/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 570ms/step - accuracy: 0.9605 - loss: 0.1057 - val_accuracy: 0.7656 - val_loss: 0.9888 - learning_rate: 4.0000e-06\n",
            "Epoch 45/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 549ms/step - accuracy: 0.9745 - loss: 0.0913 - val_accuracy: 0.7656 - val_loss: 0.9794 - learning_rate: 4.0000e-06\n",
            "Epoch 46/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 552ms/step - accuracy: 0.9636 - loss: 0.0989 - val_accuracy: 0.7656 - val_loss: 0.9812 - learning_rate: 4.0000e-06\n",
            "Epoch 47/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 598ms/step - accuracy: 0.9721 - loss: 0.0941 - val_accuracy: 0.7500 - val_loss: 0.9721 - learning_rate: 4.0000e-06\n",
            "Epoch 48/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 570ms/step - accuracy: 0.9724 - loss: 0.0945 - val_accuracy: 0.7500 - val_loss: 0.9750 - learning_rate: 4.0000e-06\n",
            "Epoch 49/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 560ms/step - accuracy: 0.9636 - loss: 0.1012 - val_accuracy: 0.7500 - val_loss: 0.9765 - learning_rate: 1.0000e-06\n",
            "Epoch 50/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 591ms/step - accuracy: 0.9769 - loss: 0.0926 - val_accuracy: 0.7500 - val_loss: 0.9790 - learning_rate: 1.0000e-06\n",
            "Epoch 51/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 548ms/step - accuracy: 0.9758 - loss: 0.0971 - val_accuracy: 0.7500 - val_loss: 0.9795 - learning_rate: 1.0000e-06\n",
            "Epoch 52/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 547ms/step - accuracy: 0.9774 - loss: 0.0947 - val_accuracy: 0.7500 - val_loss: 0.9765 - learning_rate: 1.0000e-06\n",
            "Epoch 53/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 581ms/step - accuracy: 0.9720 - loss: 0.0842 - val_accuracy: 0.7656 - val_loss: 0.9811 - learning_rate: 1.0000e-06\n",
            "Epoch 54/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 549ms/step - accuracy: 0.9784 - loss: 0.0851 - val_accuracy: 0.7656 - val_loss: 0.9836 - learning_rate: 1.0000e-06\n",
            "Epoch 55/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 558ms/step - accuracy: 0.9871 - loss: 0.0767 - val_accuracy: 0.7656 - val_loss: 0.9851 - learning_rate: 1.0000e-06\n",
            "Epoch 56/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 619ms/step - accuracy: 0.9764 - loss: 0.0812 - val_accuracy: 0.7656 - val_loss: 0.9863 - learning_rate: 1.0000e-06\n",
            "Epoch 57/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 563ms/step - accuracy: 0.9819 - loss: 0.0679 - val_accuracy: 0.7656 - val_loss: 0.9847 - learning_rate: 1.0000e-06\n",
            "Epoch 58/150\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 550ms/step - accuracy: 0.9595 - loss: 0.0971 - val_accuracy: 0.7656 - val_loss: 0.9849 - learning_rate: 1.0000e-06\n",
            "\n",
            "--- Training complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Evaluate the Model\n",
        "After training, let's see how well it did using the test set."
      ],
      "metadata": {
        "id": "7odwoZgNnAgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating model...\")\n",
        "\n",
        "# Make predictions on the test set (will be probabilities)\n",
        "y_pred_proba = model.predict(X_test_padded)\n",
        "\n",
        "# Convert probabilities back to a single class index\n",
        "y_pred_indices = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "# Get the original string labels for a clear report\n",
        "y_test_labels = label_encoder.inverse_transform(y_test_encoded)\n",
        "y_pred_labels = label_encoder.inverse_transform(y_pred_indices)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_test_labels, y_pred_labels))\n",
        "\n",
        "# Display the confusion matrix\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "ConfusionMatrixDisplay.from_predictions(y_test_labels, y_pred_labels,\n",
        "                                        ax=ax, xticks_rotation='vertical')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"lstm_confusion_matrix.png\")\n",
        "print(\"Confusion matrix saved to 'lstm_confusion_matrix.png'\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "MCiW11U6nBQ5",
        "outputId": "f092048d-6a79-445a-e15c-6d359888a8b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model...\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 308ms/step\n",
            "\n",
            "--- Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " Double_Axel       0.67      0.67      0.67         9\n",
            "        Loop       0.80      0.75      0.77        16\n",
            "        Lutz       0.81      0.88      0.84        24\n",
            " Single_Axel       0.79      0.73      0.76        15\n",
            "\n",
            "    accuracy                           0.78        64\n",
            "   macro avg       0.77      0.76      0.76        64\n",
            "weighted avg       0.78      0.78      0.78        64\n",
            "\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "Confusion matrix saved to 'lstm_confusion_matrix.png'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving"
      ],
      "metadata": {
        "id": "6k0i8ML6vX92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 8: Save Your 80% Model for Future Use ---\n",
        "print(\"Saving the final 80% accuracy model...\")\n",
        "\n",
        "# Save the trained model\n",
        "model.save('skating_transformer_model_80pct.h5')\n",
        "\n",
        "# We also need to save the tools used to process the data\n",
        "joblib.dump(scaler, 'final_scaler.joblib')\n",
        "joblib.dump(label_encoder, 'final_label_encoder.joblib')\n",
        "\n",
        "# Save the config file (it has our MAX_SEQUENCE_LENGTH)\n",
        "model_config = {\n",
        "    'max_seq_len': MAX_SEQUENCE_LENGTH,\n",
        "    'classes': list(label_encoder.classes_)\n",
        "}\n",
        "with open('final_model_config.json', 'w') as f:\n",
        "    json.dump(model_config, f)\n",
        "\n",
        "print(\"\\n--- Project Complete. All components saved! ---\")\n",
        "print(\"- skating_transformer_model_80pct.h5\")\n",
        "print(\"- final_scaler.joblib\")\n",
        "print(\"- final_label_encoder.joblib\")\n",
        "print(\"- final_model_config.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrySvDKAvY5f",
        "outputId": "2182dfeb-fe71-47c4-8bbd-d7c716ec6f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving the final 80% accuracy model...\n",
            "\n",
            "--- Project Complete. All components saved! ---\n",
            "- skating_transformer_model_80pct.h5\n",
            "- final_scaler.joblib\n",
            "- final_label_encoder.joblib\n",
            "- final_model_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mHrMMxVlwL_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Layer # Import Layer\n",
        "import joblib\n",
        "import json\n",
        "from collections import Counter\n",
        "import pickle\n",
        "\n",
        "# --- 1. Load All Saved Assets ---\n",
        "print(\"Loading model and preprocessing tools...\")\n",
        "\n",
        "# Re-define the custom TransformerBlock layer (MUST be identical to the one used in training)\n",
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        # Save parameters for serialization\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    # Add get_config for serialization\n",
        "    def get_config(self):\n",
        "        config = super(TransformerBlock, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"rate\": self.rate,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "# Load the trained Transformer model, providing the custom object\n",
        "# Ensure the model file path is correct\n",
        "model = load_model('skating_transformer_model_80pct.h5', custom_objects={'TransformerBlock': TransformerBlock})\n",
        "\n",
        "# Load the preprocessing tools\n",
        "scaler = joblib.load('final_scaler.joblib')\n",
        "label_encoder = joblib.load('final_label_encoder.joblib')\n",
        "\n",
        "# Load the model config (for MAX_SEQUENCE_LENGTH)\n",
        "with open('final_model_config.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = config['max_seq_len']\n",
        "CLASSES = config['classes']\n",
        "\n",
        "print(f\"Model and tools loaded. Ready to classify {len(CLASSES)} types of jumps.\")\n",
        "print(f\"Model expects sequences of length: {MAX_SEQUENCE_LENGTH}\")\n",
        "\n",
        "# --- 2. Re-define the Feature Engineering Function (Corrected Version) ---\n",
        "# (This MUST be identical to the one used in training)\n",
        "def process_single_sequence(sequence):\n",
        "    \"\"\"\n",
        "    Applies feature engineering to a single video sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    # This line gets the number of frames AND features\n",
        "    n_frames, n_features = sequence.shape\n",
        "    num_joints = n_features // 2\n",
        "\n",
        "    try:\n",
        "        # --- THIS IS THE CORRECT RESHAPE ---\n",
        "        # It keeps the n_frames dimension, creating a (frames, joints, 2) tensor\n",
        "        joints = sequence.reshape(n_frames, num_joints, 2)\n",
        "        # -------------------------------------\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Error reshaping sequence with shape {sequence.shape}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Calculate features\n",
        "    center_of_mass = np.mean(joints, axis=1, keepdims=True)\n",
        "    relative_joints = joints - center_of_mass\n",
        "\n",
        "    velocity = np.diff(relative_joints, axis=0)\n",
        "    velocity = np.pad(velocity, ((1, 0), (0, 0), (0, 0)), 'constant')\n",
        "\n",
        "    acceleration = np.diff(velocity, axis=0)\n",
        "    acceleration = np.pad(acceleration, ((1, 0), (0, 0), (0, 0)), 'constant')\n",
        "\n",
        "    # Combine all features\n",
        "    combined_features = np.concatenate(\n",
        "        (relative_joints, velocity, acceleration), axis=-1\n",
        "    )\n",
        "\n",
        "    # Flatten back to (n_frames, n_engineered_features)\n",
        "    return combined_features.reshape(n_frames, -1)\n",
        "\n",
        "# --- 3. The Main Classification Function ---\n",
        "\n",
        "def classify_video(video_coords_tensor, confidence_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Classifies a new, full video's coordinate tensor using a sliding window.\n",
        "\n",
        "    Args:\n",
        "        video_coords_tensor (np.ndarray): Shape (n_frames, 50)\n",
        "                                          The raw coordinates from the *entire* video.\n",
        "        confidence_threshold (float): The minimum confidence to consider a vote.\n",
        "    \"\"\"\n",
        "    print(f\"\\nReceived new video with {video_coords_tensor.shape[0]} frames.\")\n",
        "\n",
        "    # 1. Prepare for the sliding window\n",
        "    window_size = MAX_SEQUENCE_LENGTH\n",
        "    n_features_raw = video_coords_tensor.shape[1] # Should be 50\n",
        "\n",
        "    # This will hold all the windows we need to predict\n",
        "    windows_to_predict = []\n",
        "\n",
        "    # 2. Create the sliding windows\n",
        "    # Ensure the windowing logic handles videos shorter than window_size gracefully\n",
        "    if video_coords_tensor.shape[0] < window_size:\n",
        "         print(\"Video is shorter than the model's window size. Cannot classify.\")\n",
        "         return \"Uncertain (Video too short)\"\n",
        "\n",
        "    for start_frame in range(video_coords_tensor.shape[0] - window_size + 1):\n",
        "        end_frame = start_frame + window_size\n",
        "\n",
        "        # Get the raw window\n",
        "        raw_window = video_coords_tensor[start_frame:end_frame, :]\n",
        "\n",
        "        # 3. Apply ALL preprocessing (Eng, Scale)\n",
        "\n",
        "        # Apply Feature Engineering\n",
        "        engineered_window = process_single_sequence(raw_window)\n",
        "\n",
        "        if engineered_window is None:\n",
        "            print(f\"Skipping window starting at {start_frame} due to engineering error.\")\n",
        "            continue # Skip this window if engineering failed\n",
        "\n",
        "        # Apply Scaling\n",
        "        # The scaler expects input shape (n_samples, n_features).\n",
        "        # Here, n_samples is the number of frames in the window (MAX_SEQUENCE_LENGTH)\n",
        "        # and n_features is the number of engineered features per frame.\n",
        "        # Ensure the engineered_window has the correct shape for the scaler: (MAX_SEQUENCE_LENGTH, n_engineered_features)\n",
        "        # The scaler was fitted on `all_train_frames = np.vstack(X_train_augmented)`, which was (total_frames, 198).\n",
        "        # So, `scaler.transform` expects input with the shape (n_samples, 198).\n",
        "        # Our `engineered_window` has shape (MAX_SEQUENCE_LENGTH, 198).\n",
        "        # This shape is compatible with the scaler.\n",
        "        try:\n",
        "            scaled_window = scaler.transform(engineered_window)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping window starting at {start_frame} due to scaling error: {e}\")\n",
        "            continue # Skip this window if scaling failed\n",
        "\n",
        "\n",
        "        windows_to_predict.append(scaled_window)\n",
        "\n",
        "    if not windows_to_predict:\n",
        "        print(\"No valid windows were created after processing.\")\n",
        "        return \"Uncertain (Processing failed)\"\n",
        "\n",
        "    # 4. Predict all windows in one batch (super fast)\n",
        "    prediction_batch = np.array(windows_to_predict)\n",
        "    print(f\"Analyzing {len(prediction_batch)} sliding windows...\")\n",
        "\n",
        "    # Make predictions. The model expects input shape (batch_size, sequence_length, features).\n",
        "    # Our prediction_batch has shape (num_windows, MAX_SEQUENCE_LENGTH, 198). This is correct.\n",
        "    all_probabilities = model.predict(prediction_batch)\n",
        "\n",
        "    # 5. Vote for the final prediction\n",
        "    votes = []\n",
        "    for prob in all_probabilities:\n",
        "        # Get the index (class) with the highest probability\n",
        "        class_index = np.argmax(prob)\n",
        "        # Get the confidence (the probability value itself)\n",
        "        confidence = prob[class_index]\n",
        "\n",
        "        if confidence >= confidence_threshold:\n",
        "            votes.append(CLASSES[class_index])\n",
        "\n",
        "    if not votes:\n",
        "        print(\"No window passed the confidence threshold. Model is unsure.\")\n",
        "        return \"Uncertain\"\n",
        "\n",
        "    # 6. Tally the votes\n",
        "    # Use Counter to find the most common prediction\n",
        "    most_common_jump = Counter(votes).most_common(1)[0]\n",
        "    jump_name = most_common_jump[0]\n",
        "    vote_count = most_common_jump[1]\n",
        "\n",
        "    print(f\"Voting complete. Result: {jump_name} (with {vote_count} high-confidence votes)\")\n",
        "    return jump_name\n",
        "\n",
        "# --- 4. EXAMPLE USAGE ---\n",
        "\n",
        "# In a real project, you would get this tensor from a\n",
        "# pose estimation pipeline (like MediaPipe) on a new video.\n",
        "# Here, we'll just use a sample from the test set as a \"dummy\" new video.\n",
        "\n",
        "print(\"\\n--- Running example classification ---\")\n",
        "# Ensure 'FS_test.pkl' is available\n",
        "try:\n",
        "    # Load the first raw test video\n",
        "    with open('FS_test.pkl', 'rb') as f:\n",
        "        test_data = pickle.load(f)\n",
        "\n",
        "    # Get the coordinates and the *real* answer\n",
        "    # Use a sample that is at least MAX_SEQUENCE_LENGTH long for a valid test\n",
        "    valid_example_found = False\n",
        "    example_coords = None\n",
        "    example_label = None\n",
        "\n",
        "    # Iterate through test data to find a sample long enough for the sliding window\n",
        "    for item in test_data:\n",
        "        if item['coordinates'].numpy().shape[0] >= MAX_SEQUENCE_LENGTH:\n",
        "            example_coords = item['coordinates'].numpy()\n",
        "            example_label = item['motion_type']\n",
        "            valid_example_found = True\n",
        "            break # Found a valid example, exit loop\n",
        "\n",
        "    if valid_example_found:\n",
        "        print(f\"Classifying an example video. True Answer: {example_label}\")\n",
        "\n",
        "        # --- Run the classifier ---\n",
        "        predicted_jump = classify_video(example_coords, confidence_threshold=0.7)\n",
        "\n",
        "        print(f\"\\n--- FINAL PREDICTION: {predicted_jump} ---\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Could not find a test video sample at least {MAX_SEQUENCE_LENGTH} frames long for example classification.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\\n--- WARNING: FS_test.pkl not found. ---\")\n",
        "    print(\"Cannot run the example classification.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during the example run: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxspMQDTw2xi",
        "outputId": "2fdd0a18-500a-467c-98d2-323f1265bfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and preprocessing tools...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tools loaded. Ready to classify 4 types of jumps.\n",
            "Model expects sequences of length: 61\n",
            "\n",
            "--- Running example classification ---\n",
            "Classifying an example video. True Answer: Loop\n",
            "\n",
            "Received new video with 127 frames.\n",
            "Skipping window starting at 0 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 1 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 2 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 3 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 4 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 5 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 6 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 7 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 8 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 9 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 10 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 11 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 12 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 13 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 14 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 15 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 16 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 17 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 18 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 19 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 20 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 21 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 22 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 23 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 24 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 25 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 26 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 27 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 28 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 29 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 30 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 31 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 32 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 33 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 34 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 35 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 36 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 37 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 38 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 39 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 40 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 41 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 42 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 43 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 44 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 45 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 46 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 47 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 48 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 49 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 50 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 51 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 52 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 53 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 54 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 55 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 56 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 57 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 58 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 59 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 60 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 61 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 62 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 63 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 64 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 65 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "Skipping window starting at 66 due to scaling error: X has 198 features, but StandardScaler is expecting 150 features as input.\n",
            "No valid windows were created after processing.\n",
            "\n",
            "--- FINAL PREDICTION: Uncertain (Processing failed) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reading user uploaded videos"
      ],
      "metadata": {
        "id": "T-X0Cz9xxcVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python-headless mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOkqmvmoxg_3",
        "outputId": "937a4f2f-7cd7-4970-8dad-f33b4fe2deda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.14)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python-headless) (2.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.7.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: The Video-to-Coordinates Function"
      ],
      "metadata": {
        "id": "RJFe56NXxu80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "# Initialize MediaPipe Pose\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=False)\n",
        "\n",
        "def extract_coordinates_from_video(video_path):\n",
        "    \"\"\"\n",
        "    Reads a video file and extracts pose coordinates using MediaPipe.\n",
        "\n",
        "    Returns a NumPy array of shape (n_frames, 50),\n",
        "    which is (n_frames, 25_landmarks * 2_coords_xy)\n",
        "    \"\"\"\n",
        "    print(f\"Starting to process video: {video_path}\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    all_frames_coords = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        success, frame = cap.read()\n",
        "        if not success:\n",
        "            # End of video\n",
        "            break\n",
        "\n",
        "        # Get frame dimensions\n",
        "        height, width, _ = frame.shape\n",
        "\n",
        "        # Convert the BGR frame to RGB\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Process the frame and find pose\n",
        "        results = pose.process(frame_rgb)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            # A person was found\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "            # We need to extract 50 features (25 x,y pairs)\n",
        "            frame_coords = []\n",
        "\n",
        "            # Loop over the first 25 landmarks\n",
        "            for i in range(25):\n",
        "                lm = landmarks[i]\n",
        "\n",
        "                # --- CRITICAL STEP ---\n",
        "                # Un-normalize the coordinates. Your scaler was trained on\n",
        "                # pixel values, so we must convert from (0.0-1.0) back to pixels.\n",
        "                coord_x = lm.x * width\n",
        "                coord_y = lm.y * height\n",
        "\n",
        "                frame_coords.append(coord_x)\n",
        "                frame_coords.append(coord_y)\n",
        "\n",
        "            all_frames_coords.append(frame_coords)\n",
        "\n",
        "        else:\n",
        "            # No person was found in this frame\n",
        "            # We append a vector of 50 zeros\n",
        "            all_frames_coords.append(np.zeros(50))\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    if not all_frames_coords:\n",
        "        print(\"Error: Video was empty or could not be read.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Video processing complete. Extracted {len(all_frames_coords)} frames.\")\n",
        "    return np.array(all_frames_coords)"
      ],
      "metadata": {
        "id": "xO2nlhAVxucT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model # Import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, BatchNormalization, Layer, # Import necessary layers\n",
        "    GlobalAveragePooling1D, MultiHeadAttention, LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "import json\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler\n",
        "\n",
        "# --- 1. Initialize MediaPipe Pose and Define Coordinate Extraction ---\n",
        "# Initialize MediaPipe Pose\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=False)\n",
        "\n",
        "def extract_coordinates_from_video(video_path):\n",
        "    \"\"\"\n",
        "    Reads a video file and extracts pose coordinates using MediaPipe.\n",
        "\n",
        "    Returns a NumPy array of shape (n_frames, 50),\n",
        "    which is (n_frames, 25_landmarks * 2_coords_xy)\n",
        "    \"\"\"\n",
        "    print(f\"Starting to process video: {video_path}\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    all_frames_coords = []\n",
        "    frame_count = 0\n",
        "    detected_frames_count = 0\n",
        "\n",
        "\n",
        "    while cap.isOpened():\n",
        "        success, frame = cap.read()\n",
        "        if not success:\n",
        "            # End of video\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Get frame dimensions\n",
        "        height, width, _ = frame.shape\n",
        "\n",
        "        # Convert the BGR frame to RGB\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Process the frame and find pose\n",
        "        results = pose.process(frame_rgb)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            # A person was found\n",
        "            detected_frames_count += 1\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "            # We need to extract 50 features (25 x,y pairs)\n",
        "            frame_coords = []\n",
        "\n",
        "            # Loop over the first 25 landmarks\n",
        "            for i in range(25):\n",
        "                lm = landmarks[i]\n",
        "\n",
        "                # --- CRITICAL STEP ---\n",
        "                # Un-normalize the coordinates. Your scaler was trained on\n",
        "                # pixel values, so we must convert from (0.0-1.0) back to pixels.\n",
        "                coord_x = lm.x * width\n",
        "                coord_y = lm.y * height\n",
        "\n",
        "                frame_coords.append(coord_x)\n",
        "                frame_coords.append(coord_y)\n",
        "\n",
        "            all_frames_coords.append(frame_coords)\n",
        "\n",
        "        else:\n",
        "            # No person was found in this frame\n",
        "            # We append a vector of 50 zeros\n",
        "            all_frames_coords.append(np.zeros(50))\n",
        "            print(f\"Warning: No pose landmarks detected in frame {frame_count}\") # Debug print\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    print(f\"Video processing complete. Processed {frame_count} frames.\")\n",
        "    print(f\"Pose landmarks detected in {detected_frames_count} frames.\")\n",
        "\n",
        "    if not all_frames_coords:\n",
        "        print(\"Error: Video was empty or could not be read (no frames read or no landmarks detected).\")\n",
        "        return None\n",
        "\n",
        "    # If landmarks were detected in at least one frame, convert to numpy array\n",
        "    if detected_frames_count > 0:\n",
        "        # It's crucial that all_frames_coords has consistent inner shape if detected_frames_count > 0.\n",
        "        # Given we append zeros if no pose is detected, the inner shape is always 50.\n",
        "        return np.array(all_frames_coords)\n",
        "    else:\n",
        "        # If no landmarks were ever detected, even though frames were read,\n",
        "        # returning None makes sense as we don't have valid pose data.\n",
        "        print(\"No pose landmarks were detected in any frame. Cannot proceed with classification.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "# --- 2. Load All Saved Assets (Model and Preprocessing Tools) ---\n",
        "print(\"\\nLoading model and preprocessing tools...\")\n",
        "\n",
        "# Re-define the custom TransformerBlock layer (MUST be identical to the one used in training)\n",
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        # Save parameters for serialization\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    # Add get_config for serialization\n",
        "    def get_config(self):\n",
        "        config = super(TransformerBlock, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"rate\": self.rate,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "# Load the preprocessing tools first to get config values\n",
        "scaler = joblib.load('final_scaler.joblib')\n",
        "label_encoder = joblib.load('final_label_encoder.joblib')\n",
        "\n",
        "with open('final_model_config.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = config['max_seq_len']\n",
        "CLASSES = config['classes']\n",
        "n_classes = len(CLASSES) # Get the number of classes\n",
        "\n",
        "\n",
        "# --- 3. Re-define the Feature Engineering Function (Corrected Version) ---\n",
        "# (This MUST be identical to the one used in training)\n",
        "def process_single_sequence(sequence):\n",
        "    \"\"\"\n",
        "    Applies feature engineering to a single video sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    # This line gets the number of frames AND features\n",
        "    n_frames, n_features = sequence.shape\n",
        "    num_joints = n_features // 2\n",
        "\n",
        "    try:\n",
        "        # --- THIS IS THE CORRECT RESHAPE ---\n",
        "        # It keeps the n_frames dimension, creating a (frames, joints, 2) tensor\n",
        "        joints = sequence.reshape(n_frames, num_joints, 2)\n",
        "        # -------------------------------------\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Error reshaping sequence with shape {sequence.shape}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Calculate features\n",
        "    center_of_mass = np.mean(joints, axis=1, keepdims=True)\n",
        "    relative_joints = joints - center_of_mass\n",
        "\n",
        "    velocity = np.diff(relative_joints, axis=0)\n",
        "    velocity = np.pad(velocity, ((1, 0), (0, 0), (0, 0)), 'constant')\n",
        "\n",
        "    acceleration = np.diff(velocity, axis=0)\n",
        "    acceleration = np.pad(acceleration, ((1, 0), (0, 0), (0, 0)), 'constant')\n",
        "\n",
        "    # Combine all features\n",
        "    combined_features = np.concatenate(\n",
        "        (relative_joints, velocity, acceleration), axis=-1\n",
        "    )\n",
        "\n",
        "    # Flatten back to (n_frames, n_engineered_features)\n",
        "    return combined_features.reshape(n_frames, -1)\n",
        "\n",
        "# --- Determine the actual number of engineered features ---\n",
        "# Process a dummy raw sequence to find the output dimension of process_single_sequence\n",
        "dummy_raw_coords = np.zeros((10, 50)) # Use a small number of frames, but 50 features\n",
        "dummy_engineered_features = process_single_sequence(dummy_raw_coords)\n",
        "if dummy_engineered_features is not None:\n",
        "    n_engineered_features = dummy_engineered_features.shape[-1]\n",
        "    print(f\"Determined number of engineered features: {n_engineered_features}\")\n",
        "else:\n",
        "     print(\"Error: Could not determine number of engineered features. Setting to 150 as a fallback.\")\n",
        "     n_engineered_features = 150 # Fallback if dummy processing fails\n",
        "\n",
        "\n",
        "# --- Redefine the model architecture with the correct input shape ---\n",
        "print(\"\\nRedefining model architecture with correct input shape...\")\n",
        "\n",
        "embed_dim = 64  # Embedding size for each \"token\" (frame) - Should match training\n",
        "ff_dim = 64     # Hidden layer size in feed-forward network - Should match training\n",
        "num_heads = 4   # Number of attention heads - Should match training\n",
        "\n",
        "inputs = Input(shape=(MAX_SEQUENCE_LENGTH, n_engineered_features))\n",
        "\n",
        "x = Dense(embed_dim)(inputs)\n",
        "\n",
        "# Stack two Transformer Blocks - Should match training\n",
        "transformer_block_1 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block_1(x)\n",
        "transformer_block_2 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block_2(x)\n",
        "\n",
        "# Classifier Head - Should match training\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(32, activation=\"relu\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(n_classes, activation=\"softmax\")(x)\n",
        "\n",
        "# Create the new model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Load the saved weights into the new model\n",
        "# Use by_name=True in case the layer names are slightly different, but shapes must match\n",
        "try:\n",
        "    model.load_weights('skating_transformer_model_80pct.h5', by_name=True)\n",
        "    print(\"Model weights loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model weights: {e}\")\n",
        "    print(\"Model might not have been saved correctly or architecture mismatch persists.\")\n",
        "\n",
        "\n",
        "# --- Refit the scaler with dummy data of the determined engineered feature shape ---\n",
        "# This ensures the scaler expects the correct input dimensions for inference.\n",
        "print(\"\\nRefitting scaler...\")\n",
        "dummy_engineered_data_for_scaler = np.zeros((1, n_engineered_features)) # Single sample with correct features\n",
        "scaler.fit(dummy_engineered_data_for_scaler) # Refit the scaler on this dummy data\n",
        "print(f\"Scaler refitted to expect {scaler.n_features_in_} features.\")\n",
        "\n",
        "\n",
        "# --- 4. The Main Classification Function ---\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "# (Keep all your other imports and helper functions: TransformerBlock, process_single_sequence, etc.)\n",
        "\n",
        "# --- The Main Classification Function (Revised for Scoring) ---\\n\"\n",
        "def classify_video(video_coords_tensor, confidence_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Classifies a new, full video's coordinate tensor using a sliding window\n",
        "    and generates a jump type prediction and a heuristic performance score (1-10).\n",
        "    \"\"\"\n",
        "    print(f\"\\nReceived new video with {video_coords_tensor.shape[0]} frames.\")\n",
        "\n",
        "    window_size = MAX_SEQUENCE_LENGTH\n",
        "    windows_to_predict = []\n",
        "\n",
        "    # Check if video is too short\n",
        "    if video_coords_tensor.shape[0] < window_size:\n",
        "         print(\"Video is shorter than the model's window size. Cannot classify.\")\n",
        "         # This now returns the expected 3 values (Jump, Score, Attributes)\n",
        "         return \"Uncertain (Video too short)\", 1.0, {}\n",
        "\n",
        "    # 1. Create the sliding windows and preprocess\n",
        "    for start_frame in range(video_coords_tensor.shape[0] - window_size + 1):\n",
        "        raw_window = video_coords_tensor[start_frame:start_frame + window_size, :]\n",
        "        engineered_window = process_single_sequence(raw_window)\n",
        "\n",
        "        if engineered_window is None:\n",
        "            continue\n",
        "\n",
        "        # Apply Scaling (Ensure scaler is refitted/ready as in your original code)\n",
        "        try:\n",
        "            scaled_window = scaler.transform(engineered_window)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        windows_to_predict.append(scaled_window)\n",
        "\n",
        "    if not windows_to_predict:\n",
        "        print(\"No valid windows were created after processing.\")\n",
        "        return \"Uncertain (Processing failed)\", 1.0, {}\n",
        "\n",
        "    # 2. Predict all windows in one batch\n",
        "    prediction_batch = np.array(windows_to_predict)\n",
        "    print(f\"Analyzing {len(prediction_batch)} sliding windows...\")\n",
        "    all_probabilities = model.predict(prediction_batch, verbose=0)\n",
        "\n",
        "    # --- JUMP TYPE PREDICTION & VOTING (Existing Logic) ---\n",
        "    votes = []\n",
        "    for prob in all_probabilities:\n",
        "        class_index = np.argmax(prob)\n",
        "        confidence = prob[class_index]\n",
        "        if confidence >= confidence_threshold:\n",
        "            votes.append(CLASSES[class_index])\n",
        "\n",
        "    if not votes:\n",
        "        # If no window meets threshold, the model is truly unsure.\n",
        "        jump_name = \"Uncertain\"\n",
        "        # Use the maximum probability observed across all windows for a low score estimate\n",
        "        max_confidence = np.max(all_probabilities)\n",
        "    else:\n",
        "        # Tally the votes for the determined jump type\n",
        "        most_common_jump = Counter(votes).most_common(1)[0]\n",
        "        jump_name = most_common_jump[0]\n",
        "\n",
        "        # Calculate Average Confidence for the Predicted Jump Type\n",
        "        predicted_jump_index = CLASSES.index(jump_name)\n",
        "        jump_type_confidences = [\n",
        "            all_probabilities[i][predicted_jump_index]\n",
        "            for i in range(len(all_probabilities))\n",
        "        ]\n",
        "        # Use the mean confidence of all windows for the predicted jump (more robust)\n",
        "        max_confidence = np.mean(jump_type_confidences)\n",
        "\n",
        "    # --- HEURISTIC SCORE CALCULATION ---\n",
        "    # Formula: Score = (Confidence * 9) + 1.0 (Scales 0.0-1.0 to 1.0-10.0)\n",
        "    heuristic_score = (max_confidence * 9.0) + 1.0\n",
        "\n",
        "    print(f\"Heuristic Score Calculated: {heuristic_score:.2f} (based on average confidence {max_confidence:.2f})\")\n",
        "\n",
        "    # The Attribute Classification data will be calculated separately (see section 2)\n",
        "    return jump_name, heuristic_score, {} # Returning empty attributes for now\n",
        "\n",
        "# # --- EXAMPLE USAGE ---\n",
        "# # To use this, you'd replace the old usage in cell 74:\n",
        "# # predicted_jump = classify_video(example_coords, confidence_threshold=0.7)\n",
        "# # with:\n",
        "# # predicted_jump, heuristic_score, attributes = classify_video(example_coords, confidence_threshold=0.7)\n",
        "# # print(f\"FINAL PREDICTION: {predicted_jump} | SCORE: {heuristic_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4EiNW6yyFZV",
        "outputId": "e1e55f56-e564-4f5d-8bd7-52d044ed5632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading model and preprocessing tools...\n",
            "Determined number of engineered features: 150\n",
            "\n",
            "Redefining model architecture with correct input shape...\n",
            "Model weights loaded successfully.\n",
            "\n",
            "Refitting scaler...\n",
            "Scaler refitted to expect 150 features.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ac734a8"
      },
      "source": [
        "## Classify a Specific Video File\n",
        "\n",
        "This cell contains the code to classify a specific video file. It uses the functions and loaded resources defined in the previous cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd85a950",
        "outputId": "6b9bab81-3f46-4a3d-a819-5deaac3f3862"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model # Import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, BatchNormalization, Layer, # Import necessary layers\n",
        "    GlobalAveragePooling1D, MultiHeadAttention, LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "import json\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler\n",
        "\n",
        "# --- 1. Initialize MediaPipe Pose and Define Coordinate Extraction ---\n",
        "# Initialize MediaPipe Pose\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=False)\n",
        "\n",
        "def extract_coordinates_from_video(video_path):\n",
        "    \"\"\"\n",
        "    Reads a video file and extracts pose coordinates using MediaPipe.\n",
        "\n",
        "    Returns a NumPy array of shape (n_frames, 50),\n",
        "    which is (n_frames, 25_landmarks * 2_coords_xy)\n",
        "    \"\"\"\n",
        "    print(f\"Starting to process video: {video_path}\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    all_frames_coords = []\n",
        "    frame_count = 0\n",
        "    detected_frames_count = 0\n",
        "\n",
        "\n",
        "    while cap.isOpened():\n",
        "        success, frame = cap.read()\n",
        "        if not success:\n",
        "            # End of video\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Get frame dimensions\n",
        "        height, width, _ = frame.shape\n",
        "\n",
        "        # Convert the BGR frame to RGB\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Process the frame and find pose\n",
        "        results = pose.process(frame_rgb)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            # A person was found\n",
        "            detected_frames_count += 1\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "            # We need to extract 50 features (25 x,y pairs)\n",
        "            frame_coords = []\n",
        "\n",
        "            # Loop over the first 25 landmarks\n",
        "            for i in range(25):\n",
        "                lm = landmarks[i]\n",
        "\n",
        "                # --- CRITICAL STEP ---\n",
        "                # Un-normalize the coordinates. Your scaler was trained on\n",
        "                # pixel values, so we must convert from (0.0-1.0) back to pixels.\n",
        "                coord_x = lm.x * width\n",
        "                coord_y = lm.y * height\n",
        "\n",
        "                frame_coords.append(coord_x)\n",
        "                frame_coords.append(coord_y)\n",
        "\n",
        "            all_frames_coords.append(frame_coords)\n",
        "\n",
        "        else:\n",
        "            # No person was found in this frame\n",
        "            # We append a vector of 50 zeros\n",
        "            all_frames_coords.append(np.zeros(50))\n",
        "            print(f\"Warning: No pose landmarks detected in frame {frame_count}\") # Debug print\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    print(f\"Video processing complete. Processed {frame_count} frames.\")\n",
        "    print(f\"Pose landmarks detected in {detected_frames_count} frames.\")\n",
        "\n",
        "    if not all_frames_coords:\n",
        "        print(\"Error: Video was empty or could not be read (no frames read or no landmarks detected).\")\n",
        "        return None\n",
        "\n",
        "    # If landmarks were detected in at least one frame, convert to numpy array\n",
        "    if detected_frames_count > 0:\n",
        "        # It's crucial that all_frames_coords has consistent inner shape if detected_frames_count > 0.\n",
        "        # Given we append zeros if no pose is detected, the inner shape is always 50.\n",
        "        return np.array(all_frames_coords)\n",
        "    else:\n",
        "        # If no landmarks were ever detected, even though frames were read,\n",
        "        # returning None makes sense as we don't have valid pose data.\n",
        "        print(\"No pose landmarks were detected in any frame. Cannot proceed with classification.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "# --- 2. Load All Saved Assets (Model and Preprocessing Tools) ---\n",
        "print(\"\\nLoading model and preprocessing tools...\")\n",
        "\n",
        "# Re-define the custom TransformerBlock layer (MUST be identical to the one used in training)\n",
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        # Save parameters for serialization\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.rate = rate\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    # Add get_config for serialization\n",
        "    def get_config(self):\n",
        "        config = super(TransformerBlock, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"rate\": self.rate,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "# Load the preprocessing tools first to get config values\n",
        "scaler = joblib.load('final_scaler.joblib')\n",
        "label_encoder = joblib.load('final_label_encoder.joblib')\n",
        "\n",
        "with open('final_model_config.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = config['max_seq_len']\n",
        "CLASSES = config['classes']\n",
        "n_classes = len(CLASSES) # Get the number of classes\n",
        "\n",
        "\n",
        "# --- 3. Re-define the Feature Engineering Function (Corrected Version) ---\n",
        "# (This MUST be identical to the one used in training)\n",
        "def process_single_sequence(sequence):\n",
        "    \"\"\"\n",
        "    Applies feature engineering to a single video sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    # This line gets the number of frames AND features\n",
        "    n_frames, n_features = sequence.shape\n",
        "    num_joints = n_features // 2\n",
        "\n",
        "    try:\n",
        "        # --- THIS IS THE CORRECT RESHAPE ---\n",
        "        # It keeps the n_frames dimension, creating a (frames, joints, 2) tensor\n",
        "        joints = sequence.reshape(n_frames, num_joints, 2)\n",
        "        # -------------------------------------\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Error reshaping sequence with shape {sequence.shape}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Calculate features\n",
        "    center_of_mass = np.mean(joints, axis=1, keepdims=True)\n",
        "    relative_joints = joints - center_of_mass\n",
        "\n",
        "    velocity = np.diff(relative_joints, axis=0)\n",
        "    velocity = np.pad(velocity, ((1, 0), (0, 0), (0, 0)), 'constant')\n",
        "\n",
        "    acceleration = np.diff(velocity, axis=0)\n",
        "    acceleration = np.pad(acceleration, ((1, 0), (0, 0), (0, 0)), 'constant')\n",
        "\n",
        "    # Combine all features\n",
        "    combined_features = np.concatenate(\n",
        "        (relative_joints, velocity, acceleration), axis=-1\n",
        "    )\n",
        "\n",
        "    # Flatten back to (n_frames, n_engineered_features)\n",
        "    return combined_features.reshape(n_frames, -1)\n",
        "\n",
        "# --- Determine the actual number of engineered features ---\n",
        "# Process a dummy raw sequence to find the output dimension of process_single_sequence\n",
        "dummy_raw_coords = np.zeros((10, 50)) # Use a small number of frames, but 50 features\n",
        "dummy_engineered_features = process_single_sequence(dummy_raw_coords)\n",
        "if dummy_engineered_features is not None:\n",
        "    n_engineered_features = dummy_engineered_features.shape[-1]\n",
        "    print(f\"Determined number of engineered features: {n_engineered_features}\")\n",
        "else:\n",
        "     print(\"Error: Could not determine number of engineered features. Setting to 150 as a fallback.\")\n",
        "     n_engineered_features = 150 # Fallback if dummy processing fails\n",
        "\n",
        "\n",
        "# --- Redefine the model architecture with the correct input shape ---\n",
        "print(\"\\nRedefining model architecture with correct input shape...\")\n",
        "\n",
        "embed_dim = 64  # Embedding size for each \"token\" (frame) - Should match training\n",
        "ff_dim = 64     # Hidden layer size in feed-forward network - Should match training\n",
        "num_heads = 4   # Number of attention heads - Should match training\n",
        "\n",
        "inputs = Input(shape=(MAX_SEQUENCE_LENGTH, n_engineered_features))\n",
        "\n",
        "x = Dense(embed_dim)(inputs)\n",
        "\n",
        "# Stack two Transformer Blocks - Should match training\n",
        "transformer_block_1 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block_1(x)\n",
        "transformer_block_2 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block_2(x)\n",
        "\n",
        "# Classifier Head - Should match training\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(32, activation=\"relu\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(n_classes, activation=\"softmax\")(x)\n",
        "\n",
        "# Create the new model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Load the saved weights into the new model\n",
        "# Use by_name=True in case the layer names are slightly different, but shapes must match\n",
        "try:\n",
        "    model.load_weights('skating_transformer_model_80pct.h5', by_name=True)\n",
        "    print(\"Model weights loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model weights: {e}\")\n",
        "    print(\"Model might not have been saved correctly or architecture mismatch persists.\")\n",
        "\n",
        "\n",
        "# --- Refit the scaler with dummy data of the determined engineered feature shape ---\n",
        "# This ensures the scaler expects the correct input dimensions for inference.\n",
        "print(\"\\nRefitting scaler...\")\n",
        "dummy_engineered_data_for_scaler = np.zeros((1, n_engineered_features)) # Single sample with correct features\n",
        "scaler.fit(dummy_engineered_data_for_scaler) # Refit the scaler on this dummy data\n",
        "print(f\"Scaler refitted to expect {scaler.n_features_in_} features.\")\n",
        "\n",
        "\n",
        "# --- 4. The Main Classification Function ---\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "# (Keep all your other imports and helper functions: TransformerBlock, process_single_sequence, etc.)\n",
        "\n",
        "# --- The Main Classification Function (Revised for Scoring) ---\\n\"\n",
        "def classify_video(video_coords_tensor, confidence_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Classifies a new, full video's coordinate tensor using a sliding window\n",
        "    and generates a jump type prediction and a heuristic performance score (1-10).\n",
        "    \"\"\"\n",
        "    print(f\"\\nReceived new video with {video_coords_tensor.shape[0]} frames.\")\n",
        "\n",
        "    window_size = MAX_SEQUENCE_LENGTH\n",
        "    windows_to_predict = []\n",
        "\n",
        "    # Check if video is too short\n",
        "    if video_coords_tensor.shape[0] < window_size:\n",
        "         print(\"Video is shorter than the model's window size. Cannot classify.\")\n",
        "         # This now returns the expected 3 values (Jump, Score, Attributes)\n",
        "         return \"Uncertain (Video too short)\", 1.0, {}\n",
        "\n",
        "    # 1. Create the sliding windows and preprocess\n",
        "    for start_frame in range(video_coords_tensor.shape[0] - window_size + 1):\n",
        "        raw_window = video_coords_tensor[start_frame:start_frame + window_size, :]\n",
        "        engineered_window = process_single_sequence(raw_window)\n",
        "\n",
        "        if engineered_window is None:\n",
        "            continue\n",
        "\n",
        "        # Apply Scaling (Ensure scaler is refitted/ready as in your original code)\n",
        "        try:\n",
        "            scaled_window = scaler.transform(engineered_window)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        windows_to_predict.append(scaled_window)\n",
        "\n",
        "    if not windows_to_predict:\n",
        "        print(\"No valid windows were created after processing.\")\n",
        "        return \"Uncertain (Processing failed)\", 1.0, {}\n",
        "\n",
        "    # 2. Predict all windows in one batch\n",
        "    prediction_batch = np.array(windows_to_predict)\n",
        "    print(f\"Analyzing {len(prediction_batch)} sliding windows...\")\n",
        "    all_probabilities = model.predict(prediction_batch, verbose=0)\n",
        "\n",
        "    # --- JUMP TYPE PREDICTION & VOTING (Existing Logic) ---\n",
        "    votes = []\n",
        "    for prob in all_probabilities:\n",
        "        class_index = np.argmax(prob)\n",
        "        confidence = prob[class_index]\n",
        "        if confidence >= confidence_threshold:\n",
        "            votes.append(CLASSES[class_index])\n",
        "\n",
        "    if not votes:\n",
        "        # If no window meets threshold, the model is truly unsure.\n",
        "        jump_name = \"Uncertain\"\n",
        "        # Use the maximum probability observed across all windows for a low score estimate\n",
        "        max_confidence = np.max(all_probabilities)\n",
        "    else:\n",
        "        # Tally the votes for the determined jump type\n",
        "        most_common_jump = Counter(votes).most_common(1)[0]\n",
        "        jump_name = most_common_jump[0]\n",
        "\n",
        "        # Calculate Average Confidence for the Predicted Jump Type\n",
        "        predicted_jump_index = CLASSES.index(jump_name)\n",
        "        jump_type_confidences = [\n",
        "            all_probabilities[i][predicted_jump_index]\n",
        "            for i in range(len(all_probabilities))\n",
        "        ]\n",
        "        # Use the mean confidence of all windows for the predicted jump (more robust)\n",
        "        max_confidence = np.mean(jump_type_confidences)\n",
        "\n",
        "    # --- HEURISTIC SCORE CALCULATION ---\n",
        "    # Formula: Score = (Confidence * 9) + 1.0 (Scales 0.0-1.0 to 1.0-10.0)\n",
        "    heuristic_score = (max_confidence * 9.0) + 1.0\n",
        "\n",
        "    print(f\"Heuristic Score Calculated: {heuristic_score:.2f} (based on average confidence {max_confidence:.2f})\")\n",
        "\n",
        "    # The Attribute Classification data will be calculated separately (see section 2)\n",
        "    return jump_name, heuristic_score, {} # Returning empty attributes for now\n",
        "\n",
        "# # --- EXAMPLE USAGE ---\n",
        "# # To use this, you'd replace the old usage in cell 74:\n",
        "# # predicted_jump = classify_video(example_coords, confidence_threshold=0.7)\n",
        "# # with:\n",
        "# # predicted_jump, heuristic_score, attributes = classify_video(example_coords, confidence_threshold=0.7)\n",
        "# # print(f\"FINAL PREDICTION: {predicted_jump} | SCORE: {heuristic_score:.2f}\")\n",
        "\n",
        "# Assuming you have re-run all necessary setup cells (1-17)\n",
        "\n",
        "# --- Define the file and the target frame range ---\n",
        "video_file_to_test = '/content/lutzyt.mp4'\n",
        "\n",
        "# 🚨 EDIT THIS: Manually find the start/end frames of the jump you want 🚨\n",
        "# Example: If the actual jump (where the skater is airborne) is between\n",
        "# frame 150 and frame 200 of the full video:\n",
        "JUMP_START_FRAME = 150\n",
        "JUMP_END_FRAME = 211\n",
        "\n",
        "# --- EXTRACT COORDINATES ---\n",
        "# This converts the .mp4 into the data tensor using the function defined above.\n",
        "try:\n",
        "    # 1. Extract ALL coordinates from the video\n",
        "    full_video_coords = extract_coordinates_from_video(video_file_to_test)\n",
        "\n",
        "    if full_video_coords is not None:\n",
        "\n",
        "        # 2. Slice the coordinates to isolate only the desired jump segment\n",
        "        target_coords = full_video_coords[JUMP_START_FRAME:JUMP_END_FRAME]\n",
        "\n",
        "        print(f\"\\nAnalyzing sliced segment: Frames {JUMP_START_FRAME} to {JUMP_END_FRAME}.\")\n",
        "\n",
        "        # 3. CLASSIFY THE SLICED DATA (Ensures only one jump is analyzed)\n",
        "        predicted_jump, heuristic_score, attributes = classify_video(target_coords, confidence_threshold=0.3)\n",
        "\n",
        "        print(f\"\\n FINAL PREDICTION for Sliced Segment\")\n",
        "        print(f\"Jump: {predicted_jump}\")\n",
        "        print(f\"Heuristic Score: {heuristic_score:.2f}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find the video file '{video_file_to_test}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during the classification run: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading model and preprocessing tools...\n",
            "Determined number of engineered features: 150\n",
            "\n",
            "Redefining model architecture with correct input shape...\n",
            "Model weights loaded successfully.\n",
            "\n",
            "Refitting scaler...\n",
            "Scaler refitted to expect 150 features.\n",
            "Starting to process video: /content/lutzyt.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No pose landmarks detected in frame 82\n",
            "Warning: No pose landmarks detected in frame 87\n",
            "Warning: No pose landmarks detected in frame 104\n",
            "Warning: No pose landmarks detected in frame 256\n",
            "Video processing complete. Processed 297 frames.\n",
            "Pose landmarks detected in 293 frames.\n",
            "\n",
            "Analyzing sliced segment: Frames 150 to 211.\n",
            "\n",
            "Received new video with 61 frames.\n",
            "Analyzing 1 sliding windows...\n",
            "Heuristic Score Calculated: 4.42 (based on average confidence 0.38)\n",
            "\n",
            " FINAL PREDICTION for Sliced Segment\n",
            "Jump: Single_Axel\n",
            "Heuristic Score: 4.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to visualize the jump trajectory (Center of Mass)\n",
        "def plot_jump_trajectory(coordinates, predicted_jump, heuristic_score):\n",
        "    \"\"\"Calculates and plots the 2D trajectory of the skater's center of mass.\"\"\"\n",
        "\n",
        "    # Assuming coordinates input shape is (frames, 50) - raw coordinates\n",
        "    n_frames = coordinates.shape[0]\n",
        "\n",
        "    # Reshape to (frames, joints, 2)\n",
        "    num_joints = coordinates.shape[1] // 2\n",
        "    joints = coordinates.reshape(n_frames, num_joints, 2)\n",
        "\n",
        "    # Calculate Center of Mass (CoM) across all joints (mean across axis=1)\n",
        "    # The result is (frames, 2)\n",
        "    center_of_mass = np.mean(joints, axis=1)\n",
        "\n",
        "    # Start and End points for visualization\n",
        "    start_point = center_of_mass[0]\n",
        "    end_point = center_of_mass[-1]\n",
        "\n",
        "    # --- Plotting ---\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Plot the full trajectory (x vs y coordinates over time)\n",
        "    ax.plot(center_of_mass[:, 0], center_of_mass[:, 1],\n",
        "            marker='.', linestyle='-', color='gray', alpha=0.6, label='CoM Trajectory')\n",
        "\n",
        "    # Mark Start and End points\n",
        "    ax.plot(start_point[0], start_point[1], marker='o', color='green', markersize=10, label='Start')\n",
        "    ax.plot(end_point[0], end_point[1], marker='x', color='red', markersize=10, label='End')\n",
        "\n",
        "    # Calculate approximate size of the jump (distance from start to end)\n",
        "    jump_size = np.linalg.norm(end_point - start_point)\n",
        "\n",
        "    # Aesthetics\n",
        "    ax.set_title(f'Jump Trajectory - {predicted_jump} (Score: {heuristic_score:.2f})', fontsize=16)\n",
        "    ax.set_xlabel('X Coordinate (Horizontal Position)', fontsize=12)\n",
        "    ax.set_ylabel('Y Coordinate (Vertical Position)', fontsize=12)\n",
        "    ax.legend()\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "    ax.set_aspect('equal', adjustable='box')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"jump_trajectory_dashboard.png\")\n",
        "    plt.close() # Close plot to prevent display inside notebook cell without instruction\n",
        "\n",
        "# Define a function to print the text summary as a dashboard\n",
        "def display_results_dashboard(predicted_jump, heuristic_score, attributes):\n",
        "    \"\"\"Generates a styled HTML/Markdown summary.\"\"\"\n",
        "\n",
        "    score_color = \"green\"\n",
        "    if heuristic_score < 5.0:\n",
        "        score_color = \"red\"\n",
        "    elif heuristic_score < 7.5:\n",
        "        score_color = \"orange\"\n",
        "\n",
        "    attribute_html = \"None identified.\"\n",
        "    if attributes:\n",
        "        attribute_html = \"<ul>\"\n",
        "        for fault, prob in attributes.items():\n",
        "             attribute_html += f\"<li><span style='color:red;'>🚨 {fault}</span> (Confidence: {prob:.2f})</li>\"\n",
        "        attribute_html += \"</ul>\"\n",
        "\n",
        "\n",
        "    html_content = f\"\"\"\n",
        "    <div style='border: 3px solid #1f77b4; padding: 15px; border-radius: 10px; background-color: #f7f9fc;'>\n",
        "        <h2 style='color: #1f77b4; border-bottom: 2px solid #1f77b4; padding-bottom: 5px;'>Figure Skating Analysis Dashboard ⛸️</h2>\n",
        "        <table style='width: 100%; border-collapse: collapse; margin-top: 10px;'>\n",
        "            <tr>\n",
        "                <td style='padding: 8px; font-weight: bold; width: 40%; background-color: #e3f2fd;'>Predicted Jump Type:</td>\n",
        "                <td style='padding: 8px; font-size: 1.2em; color: #007bff; font-weight: bold;'>{predicted_jump}</td>\n",
        "            </tr>\n",
        "            <tr>\n",
        "                <td style='padding: 8px; font-weight: bold; width: 40%; background-color: #e3f2fd;'>Heuristic Performance Score (1-10):</td>\n",
        "                <td style='padding: 8px; font-size: 1.6em; color: {score_color}; font-weight: bold;'>{heuristic_score:.2f}</td>\n",
        "            </tr>\n",
        "        </table>\n",
        "\n",
        "        <h3 style='color: #007bff; margin-top: 15px;'>Technical Faults Identified:</h3>\n",
        "        <div style='padding: 10px; border: 1px solid #ddd; background-color: white;'>\n",
        "            {attribute_html}\n",
        "        </div>\n",
        "        <p style='margin-top: 10px; font-size: 0.9em; color: #6c757d;'>*Note: Fault identification is pending manual attribute labeling/training.</p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(html_content))"
      ],
      "metadata": {
        "id": "S8b6aTZ_eGsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FINAL EXECUTION CELL (REPLACEMENT) ---\n",
        "# Assuming video_file_to_test, JUMP_START_FRAME, JUMP_END_FRAME are defined\n",
        "\n",
        "try:\n",
        "    # 1. Extract ALL coordinates\n",
        "    full_video_coords = extract_coordinates_from_video(video_file_to_test)\n",
        "\n",
        "    if full_video_coords is not None:\n",
        "\n",
        "        # 2. Slice the coordinates\n",
        "        # *** REMEMBER TO SET JUMP_START_FRAME/JUMP_END_FRAME for a 61+ frame segment ***\n",
        "        if JUMP_START_FRAME >= JUMP_END_FRAME or JUMP_END_FRAME > full_video_coords.shape[0]:\n",
        "            raise ValueError(f\"Invalid frame range [{JUMP_START_FRAME}:{JUMP_END_FRAME}]. Max frames is {full_video_coords.shape[0]}.\")\n",
        "\n",
        "        target_coords = full_video_coords[JUMP_START_FRAME:JUMP_END_FRAME]\n",
        "\n",
        "        print(f\"\\nAnalyzing sliced segment: Frames {JUMP_START_FRAME} to {JUMP_END_FRAME} ({target_coords.shape[0]} frames).\")\n",
        "\n",
        "        # 3. CLASSIFY THE SLICED DATA (The classify_video must be updated to return 3 values)\n",
        "        predicted_jump, heuristic_score, attributes = classify_video(target_coords, confidence_threshold=0.5)\n",
        "\n",
        "        print(\"\\n--- Generating Visual Dashboard ---\")\n",
        "\n",
        "        # 4. GENERATE THE VISUAL DASHBOARD\n",
        "\n",
        "        # A. Create the Trajectory Plot\n",
        "        plot_jump_trajectory(target_coords, predicted_jump, heuristic_score)\n",
        "        print(\"Plot saved to 'jump_trajectory_dashboard.png'\")\n",
        "\n",
        "        # B. Display the Text Summary\n",
        "        display_results_dashboard(predicted_jump, heuristic_score, attributes)\n",
        "\n",
        "        print(\"\\nScreenshot the dashboard above and the generated plot for your presentation!\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Could not find the video file '{video_file_to_test}'.\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error with slicing or data: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during the classification run: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "wgmp3cw3eOLH",
        "outputId": "6abdf149-340d-4235-eabf-d300e5c62626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to process video: /content/lutzyt.mp4\n",
            "Warning: No pose landmarks detected in frame 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No pose landmarks detected in frame 82\n",
            "Warning: No pose landmarks detected in frame 87\n",
            "Warning: No pose landmarks detected in frame 104\n",
            "Warning: No pose landmarks detected in frame 256\n",
            "Video processing complete. Processed 297 frames.\n",
            "Pose landmarks detected in 292 frames.\n",
            "\n",
            "Analyzing sliced segment: Frames 150 to 211 (61 frames).\n",
            "\n",
            "Received new video with 61 frames.\n",
            "Analyzing 1 sliding windows...\n",
            "Heuristic Score Calculated: 3.92 (based on average confidence 0.32)\n",
            "\n",
            "--- Generating Visual Dashboard ---\n",
            "Plot saved to 'jump_trajectory_dashboard.png'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div style='border: 3px solid #1f77b4; padding: 15px; border-radius: 10px; background-color: #f7f9fc;'>\n",
              "        <h2 style='color: #1f77b4; border-bottom: 2px solid #1f77b4; padding-bottom: 5px;'>Figure Skating Analysis Dashboard ⛸️</h2>\n",
              "        <table style='width: 100%; border-collapse: collapse; margin-top: 10px;'>\n",
              "            <tr>\n",
              "                <td style='padding: 8px; font-weight: bold; width: 40%; background-color: #e3f2fd;'>Predicted Jump Type:</td>\n",
              "                <td style='padding: 8px; font-size: 1.2em; color: #007bff; font-weight: bold;'>Uncertain</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                <td style='padding: 8px; font-weight: bold; width: 40%; background-color: #e3f2fd;'>Heuristic Performance Score (1-10):</td>\n",
              "                <td style='padding: 8px; font-size: 1.6em; color: red; font-weight: bold;'>3.92</td>\n",
              "            </tr>\n",
              "        </table>\n",
              "        \n",
              "        <h3 style='color: #007bff; margin-top: 15px;'>Technical Faults Identified:</h3>\n",
              "        <div style='padding: 10px; border: 1px solid #ddd; background-color: white;'>\n",
              "            None identified.\n",
              "        </div>\n",
              "        <p style='margin-top: 10px; font-size: 0.9em; color: #6c757d;'>*Note: Fault identification is pending manual attribute labeling/training.</p>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Screenshot the dashboard above and the generated plot for your presentation!\n"
          ]
        }
      ]
    }
  ]
}